{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages and Functions required\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling  import SMOTE\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "# Define the folder path containing CSV files\n",
    "folder_path = './../har70plus'\n",
    "\n",
    "# List to hold individual DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df_raw = pd.read_csv(file_path)\n",
    "        # Add a new column with the file name\n",
    "        df_raw['source_file'] = filename.replace('.csv','')\n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df_raw)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Convert date time to unix timestamp\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['unix_timestamp_ms'] = (df['timestamp'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1ms')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp            0\n",
       "back_x               0\n",
       "back_y               0\n",
       "back_z               0\n",
       "thigh_x              0\n",
       "thigh_y              0\n",
       "thigh_z              0\n",
       "label                0\n",
       "source_file          0\n",
       "unix_timestamp_ms    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if any null value is present\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sepetrate out the target and features\n",
    "y = df.iloc[:,7:8]\n",
    "x = df[df.columns.difference(['label','timestamp']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing sets (80% training, 20% testing) -- Add stratification as target classes are imbalanced\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Validation set not required as we have Cross Validation in Grid Search CV\n",
    "# # Splitting training set into training and validation sets (75% training, 25% validation) -- Add stratification as target classes are imbalanced\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1807677, 8), (451920, 8), (2259597, 8))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Shape------\n",
      "Shape     : (2259597, 10)\n",
      "Size      : 22595970\n",
      "Dimension : 2\n",
      "\n",
      "------Types------\n",
      "back_x               float64\n",
      "back_y               float64\n",
      "back_z               float64\n",
      "source_file           object\n",
      "thigh_x              float64\n",
      "thigh_y              float64\n",
      "thigh_z              float64\n",
      "unix_timestamp_ms      int64\n",
      "dtype: object\n",
      "\n",
      "------Head------\n",
      "           back_x    back_y    back_z source_file   thigh_x   thigh_y  \\\n",
      "268015  -0.924072 -0.091064  0.311768         503  0.312500 -0.078369   \n",
      "2087578 -0.841309 -0.079834 -0.106689         517 -0.845947 -0.010986   \n",
      "946707  -0.932129  0.003418 -0.328613         508 -0.988281  0.041748   \n",
      "1312161 -0.924561  0.027832 -0.103027         511 -1.001221 -0.132324   \n",
      "519686  -0.729492 -0.171875  0.593750         505  0.098145 -0.079590   \n",
      "\n",
      "          thigh_z  unix_timestamp_ms  \n",
      "268015  -1.047119      1617029603659  \n",
      "2087578  0.213623      1623149048927  \n",
      "946707  -0.144531      1618585773845  \n",
      "1312161 -0.077637      1620384409703  \n",
      "519686  -1.093750      1618325297399  \n",
      "\n",
      "------Tail------\n",
      "           back_x    back_y    back_z source_file   thigh_x   thigh_y  \\\n",
      "1122621 -0.027344 -0.888672  0.464355         510  0.070312  0.983398   \n",
      "921584  -1.423340 -0.195557 -0.440430         508 -1.502197 -0.200928   \n",
      "264185  -0.294922 -0.109375  0.875000         503  0.000000 -0.017578   \n",
      "1292288 -0.888672  0.065918 -0.045898         511 -0.645020  0.050293   \n",
      "1358362 -0.997070  0.056396 -0.136475         512  0.187500  0.188477   \n",
      "\n",
      "          thigh_z  unix_timestamp_ms  \n",
      "1122621  0.085449      1620376195669  \n",
      "921584  -0.383545      1618585271385  \n",
      "264185  -1.093750      1617029519060  \n",
      "1292288 -0.197266      1620384012243  \n",
      "1358362 -1.046875      1620402469779  \n",
      "\n",
      "------Missing Values------\n",
      "back_x               0\n",
      "back_y               0\n",
      "back_z               0\n",
      "source_file          0\n",
      "thigh_x              0\n",
      "thigh_y              0\n",
      "thigh_z              0\n",
      "unix_timestamp_ms    0\n",
      "dtype: int64\n",
      "\n",
      "------Duplicated Values------\n",
      "0\n",
      "\n",
      "------Unique Values------\n",
      "back_x                  9090\n",
      "back_y                  6666\n",
      "back_z                  8903\n",
      "source_file               18\n",
      "thigh_x                18756\n",
      "thigh_y                15994\n",
      "thigh_z                17272\n",
      "unix_timestamp_ms    1807677\n",
      "dtype: int64\n",
      "\n",
      "------Describe------\n",
      "                       count          mean           std           min  \\\n",
      "back_x             1807677.0 -8.699757e-01  2.689962e-01 -4.333252e+00   \n",
      "back_y             1807677.0 -3.309723e-02  1.512121e-01 -2.031006e+00   \n",
      "back_z             1807677.0  2.344575e-02  4.327259e-01 -2.066650e+00   \n",
      "thigh_x            1807677.0 -6.796836e-01  5.518655e-01 -7.907227e+00   \n",
      "thigh_y            1807677.0  2.725980e-03  2.736568e-01 -5.142578e+00   \n",
      "thigh_z            1807677.0 -3.843120e-01  5.127935e-01 -7.593750e+00   \n",
      "unix_timestamp_ms  1807677.0  1.619948e+12  2.179536e+09  1.616597e+12   \n",
      "\n",
      "                            25%           50%           75%           max  \n",
      "back_x            -9.929200e-01 -9.394530e-01 -8.281250e-01  3.630370e-01  \n",
      "back_y            -1.096190e-01 -2.172900e-02  5.078100e-02  1.576660e+00  \n",
      "back_z            -2.749020e-01 -1.125490e-01  3.125000e-01  1.177490e+00  \n",
      "thigh_x           -9.873050e-01 -9.357910e-01 -7.055700e-02  3.395264e+00  \n",
      "thigh_y           -1.166990e-01 -1.562500e-02  1.125490e-01  5.725098e+00  \n",
      "thigh_z           -9.860840e-01 -1.877440e-01 -4.150000e-03  3.953369e+00  \n",
      "unix_timestamp_ms  1.618326e+12  1.620376e+12  1.622108e+12  1.623410e+12  \n"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "def check_df(data, head=5):\n",
    "    print(\"\\n------Shape------\")\n",
    "    print(f'Shape     : {df.shape}\\n'\n",
    "          f'Size      : {df.size}\\n'\n",
    "          f'Dimension : {df.ndim}')\n",
    "    print(\"\\n------Types------\")\n",
    "    print(data.dtypes)\n",
    "    print(\"\\n------Head------\")\n",
    "    print(data.head(head))\n",
    "    print(\"\\n------Tail------\")\n",
    "    print(data.tail(head))\n",
    "    print(\"\\n------Missing Values------\")\n",
    "    print(data.isnull().sum())\n",
    "    print(\"\\n------Duplicated Values------\")\n",
    "    print(data.duplicated().sum())\n",
    "    print(\"\\n------Unique Values------\")\n",
    "    print(data.nunique())\n",
    "    print(\"\\n------Describe------\")\n",
    "    print(data.describe().T)\n",
    "\n",
    "check_df(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate out the num and cat features\n",
    "cat_cols = ['source_file']\n",
    "num_cols = [cols for cols in x.columns if cols not in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    1079312\n",
       "7     483452\n",
       "6     418055\n",
       "8     203182\n",
       "3      66058\n",
       "5       4978\n",
       "4       4560\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for imbalance in dataset\n",
    "y['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature Engineering\n",
    "\n",
    "# #use rf to get feature inportances\n",
    "# rf_classifier_fi = RandomForestClassifier(n_estimators=100, random_state=42, verbose=0)\n",
    "# rf_classifier_fi.fit(x_train, y_train)\n",
    "# feature_importance = rf_classifier_fi.feature_importances_\n",
    "# feature_importance_df = pd.DataFrame({'Feature':x_train.columns, 'Feature_importance':feature_importance}).sort_values(by='Feature_importance', ascending=False)\n",
    "# feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting fetaures who importance more tha 0.05\n",
    "x_train = x_train[x_train.columns.difference(['source_file'])]\n",
    "# x_val = x_val[x_val.columns.difference(['source_file'])]\n",
    "x_test = x_test[x_test.columns.difference(['source_file'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the training data using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "x_resampled, y_resampled = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'n_estimators': 75}\n"
     ]
    }
   ],
   "source": [
    "#Apply Random Forest to the data with Grid search CV to get optimal hyperparameters \n",
    "# Define the parameter distributions\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 75, 100],     \n",
    "    'max_depth': [None, 10, 20]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier with class weights\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3, scoring='f1_weighted')\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Apply Random Forest to the data\n",
    "# # Initialize your Random Forest model with class weights\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42, verbose=0, max_depth=10 ,class_weight='balanced')\n",
    "\n",
    "# # Train the model on the selected features\n",
    "# rf_classifier.fit(x_resampled, y_resampled)\n",
    "# # Make predictions on the validation set\n",
    "# predictions = rf_classifier.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model with validation data\n",
    "\n",
    "# #Calculate accuracy\n",
    "# accuracy = accuracy_score(y_val, predictions)\n",
    "\n",
    "# # Calculate precision\n",
    "# precision = precision_score(y_val, predictions, average='weighted')\n",
    "\n",
    "# # Calculate recall\n",
    "# recall = recall_score(y_val, predictions, average='weighted')\n",
    "\n",
    "# # Calculate F1-score\n",
    "# f1 = f1_score(y_val, predictions, average='weighted')\n",
    "\n",
    "# # Calculate ROC-AUC (for multiclass classification, you need to use one-vs-all strategy)\n",
    "# roc_auc = roc_auc_score(y_val, rf_classifier.predict_proba(x_val), average='weighted', multi_class='ovr')\n",
    "\n",
    "# # Generate confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_val, predictions)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"F1-score:\", f1)\n",
    "# print(\"ROC-AUC:\", roc_auc)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# # # Calculate class-wise accuracy\n",
    "# # class_wise_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# # # Print class-wise accuracy\n",
    "# # for i, accuracy in enumerate(class_wise_accuracy):\n",
    "# #     print(f\"Class {i} Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.954635776243583\n",
      "Precision: 0.9525493084277606\n",
      "Recall: 0.954635776243583\n",
      "F1-score: 0.9477249139335647\n",
      "ROC-AUC: 0.9953934904777091\n",
      "Confusion Matrix:\n",
      "[[212801    301     18      9   2708     26      0]\n",
      " [  7660   3168      6      2   2376      0      0]\n",
      " [   569      4    319      5     14      1      0]\n",
      " [   740      0     10    236     10      0      0]\n",
      " [  5681    311      2      0  77617      0      0]\n",
      " [    35      0      0      0      0  96653      2]\n",
      " [     0      0      0      0      0     11  40625]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model with test data\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_test = best_model.predict(x_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions_test)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, predictions_test, average='weighted')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, predictions_test, average='weighted')\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, predictions_test, average='weighted')\n",
    "\n",
    "# Calculate ROC-AUC (for multiclass classification, you need to use one-vs-all strategy)\n",
    "roc_auc = roc_auc_score(y_test, best_model.predict_proba(x_test), average='weighted', multi_class='ovr')\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predictions_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"ROC-AUC:\", roc_auc)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# # Calculate class-wise accuracy\n",
    "# class_wise_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# # Print class-wise accuracy\n",
    "# for i, accuracy in enumerate(class_wise_accuracy):\n",
    "#     print(f\"Class {i} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to HAR_Random_Forest_Model.pkl\n"
     ]
    }
   ],
   "source": [
    "#Save the trained model using pickle\n",
    "model_filename = 'HAR_Random_Forest_Model.pkl'\n",
    "with open(model_filename,'wb') as file:\n",
    "    pickle.dump(rf_classifier,file)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_classify",
   "language": "python",
   "name": "ml_classify"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
